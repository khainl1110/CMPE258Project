{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62bf35-b4b2-43f1-a08a-7bc4a680c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324ba1-732e-488f-94c5-4116625986fd",
   "metadata": {},
   "source": [
    "## Load dog breed and dog classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947df478-2079-48e7-8672-df29cc2a6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data(data_dir):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            # print(img)\n",
    "            # print(path)\n",
    "            # img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            # print(os.path.join(path,img))\n",
    "            if img_arr is None:\n",
    "                # print(f\"Warning: Unable to load image {img}\")\n",
    "                print(os.path.join(path,img))\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "            \n",
    "    return np.array(data), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8dc13f-3140-49df-b8ad-85c528cbc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data_subset(data_dir, subset_size=None):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)  # Assuming label directories are in data_dir\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        \n",
    "        # Get list of images in the directory\n",
    "        all_images = os.listdir(path)\n",
    "        \n",
    "        # If subset_size is provided, select a random subset of images\n",
    "        if subset_size is not None and len(all_images) > subset_size:\n",
    "            all_images = random.sample(all_images, subset_size)\n",
    "        \n",
    "        for img in all_images:\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            if img_arr is None:\n",
    "                print(f\"Warning: Unable to load image {img}\")\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            \n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "    \n",
    "    return np.array(data), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbc6f4a-4e17-4f04-b279-7e3a5c127e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_resnet(data):\n",
    "    # Convert teh data array to a PIL Image\n",
    "    \n",
    "    # Example: Load multiple images (let's say 3 images)\n",
    "    # image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]  # List of image file paths\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),           # Resize to 256x256 (larger than 224x224 to allow for cropping)\n",
    "        transforms.CenterCrop(224),       # Crop to 224x224\n",
    "        transforms.ToTensor(),            # Convert to tensor (will scale pixel values to [0, 1])\n",
    "        transforms.Normalize(             # Normalize with ImageNet mean and std\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Prepare the list of images\n",
    "    input_tensors = []\n",
    "    for image in data:\n",
    "        image = Image.fromarray(image)\n",
    "        image_tensor = transform(image)  # Apply transformations\n",
    "        input_tensors.append(image_tensor)\n",
    "    \n",
    "    # Stack the images into a single tensor (batch of images)\n",
    "    input_batch = torch.stack(input_tensors)  # Shape will be [batch_size, 3, 224, 224]\n",
    "    return input_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b679c2-6d7b-4f13-950b-cb1714a5f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for image categories\n",
    "labels = ['Beagle', 'Boxer', 'Bulldog', 'Dachshund', 'German_Shepherd', 'Golden_Retriever', 'Labrador_Retriever', 'Poodle', 'Rottweiler', 'Yorkshire_Terrier']\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb29e7a-9a1a-456d-bf1b-c478768e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed_train_data, dog_breed_train_label = loading_training_data('./DogBreedImageDataset/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3112deea-d087-4d3f-bd80-aa510aa039a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fdea5f-06b1-4477-a7a0-b8a0c980bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dog_breed_train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856b0137-57a5-4251-870a-0590d32d7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ed1a08-76b8-4839-8634-0accc75ba150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db817a6-1d5f-4f12-9e68-1ab91c7008f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transform_input_resnet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7fca79-d629-4b4c-8617-73f0a0e9a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb0e35b-c515-46d6-85b4-c694ac41366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['angry', 'happy', 'relaxed', 'sad']\n",
    "# Load data for training\n",
    "dog_emotion_train_data, dog_emotion_train_label = loading_training_data_subset('./DogEmotionPrediction/images/', 250) #100 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46b8c56-ee2e-447d-847a-c7fe16fe1f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106717b-04e6-40ad-87f9-204fc20b299d",
   "metadata": {},
   "source": [
    "## Combine the two together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2d2824c-7548-4632-a7c7-6a3fbcc05b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfe452f8-bd61-4abd-8bd9-1c9e08d4699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3daf7-7ad5-4ec6-89db-38450c61f16d",
   "metadata": {},
   "source": [
    "### Modify label to have two label, first label is emotion, second is dog breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dac9b8a-e625-4032-a502-6188dfa9daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed_train_label = dog_breed_train_label.reshape(-1, 1)\n",
    "\n",
    "# Create a column of zeros (same number of rows as arr_reshaped)\n",
    "emotion_col = np.full((dog_breed_train_label.shape[0], 1), 10)\n",
    "\n",
    "# Add the column of zeros to the right using np.hstack\n",
    "dog_breed_train_label1 = np.hstack((emotion_col, dog_breed_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b090977-1109-4ccb-9ba3-acd80fc8f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_emotion_train_label = dog_emotion_train_label.reshape(-1, 1)\n",
    "\n",
    "# Create a column of zeros (same number of rows as arr_reshaped)\n",
    "breed_col = np.full((dog_emotion_train_label.shape[0], 1), 10)\n",
    "\n",
    "# Add the column of zeros to the right using np.hstack\n",
    "dog_emotion_train_label1 = np.hstack((dog_emotion_train_label, breed_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "760bae57-6bbc-4615-a6ce-92deb8ffb139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_label1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7485de9-c794-4b8c-b90e-cc894b708d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [10,  0],\n",
       "       [10,  0],\n",
       "       ...,\n",
       "       [10,  9],\n",
       "       [10,  9],\n",
       "       [10,  9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57cfde72-2c2f-4cf8-bc8e-d1fc51239779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_label1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad0d8fb-bee5-40ed-becd-8ba7216aeb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 10],\n",
       "       [ 0, 10],\n",
       "       [ 0, 10],\n",
       "       ...,\n",
       "       [ 3, 10],\n",
       "       [ 3, 10],\n",
       "       [ 3, 10]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172118e1-d2c5-4c64-a208-103be5d2a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = np.concatenate((dog_breed_train_data, dog_emotion_train_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12c72f74-9fdd-4e56-9d3e-a5560eac95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_label = np.concatenate((dog_breed_train_label1, dog_emotion_train_label1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d91ac01-0297-451b-8989-b391e1479e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [10,  0],\n",
       "       [10,  0],\n",
       "       ...,\n",
       "       [ 3, 10],\n",
       "       [ 3, 10],\n",
       "       [ 3, 10]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164c9e0-edd3-488c-9abc-7405c1ec3174",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b56b2c8d-3299-4afa-9cd4-45a7035a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fea0efa2-b9f4-44c7-bee8-445f5ac4bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combined_dataset, combined_label, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17fd4f8e-c6f5-4d7e-b4f9-8df49f82399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1947, 32, 32, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "492d9cc8-62d9-41f5-bb38-5e3eefe1b333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb6d73-11d8-4666-a7fd-df59dbd72ccc",
   "metadata": {},
   "source": [
    "### Take part of data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6a940cd-61a1-4ce9-bee1-f4e5a59c8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "130312db-ff46-4578-ba08-d6d30bb25875",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X_train.shape[0], train_size, replace=False)\n",
    "\n",
    "# Use these indices to extract 50 random samples\n",
    "X_train1 = X_train[random_indices]\n",
    "y_train1 = y_train[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3847916-a0bb-4086-95a5-d9e3a3050922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 32, 32, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b20e42b-3ef6-4784-b0ec-123f75fcce51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe9a4e01-0105-4d4d-a9b4-615a741ac80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282e771-d8fa-4ed8-9428-244f59cb8551",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d81dc99a-2926-44f0-a2e2-453d1b345109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MultiTaskResNet50(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2):\n",
    "        super(MultiTaskResNet50, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50()\n",
    "        \n",
    "        # Remove the final fully connected layer (classifier) for both tasks\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Exclude final FC layer\n",
    "        \n",
    "        # Task 1: A new fully connected layer for the first task\n",
    "        self.fc_task1 = nn.Linear(2048, num_classes_task1)\n",
    "        \n",
    "        # Task 2: A new fully connected layer for the second task\n",
    "        self.fc_task2 = nn.Linear(2048, num_classes_task2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through the ResNet backbone (everything before the final fully connected layer)\n",
    "        features = self.resnet(x)  # Output size: [batch_size, 2048, 1, 1]\n",
    "        \n",
    "        # Flatten the output to [batch_size, 2048]\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Task 1: Pass through the task-specific classifier\n",
    "        task1_output = self.fc_task1(features)\n",
    "        \n",
    "        # Task 2: Pass through the task-specific classifier\n",
    "        task2_output = self.fc_task2(features)\n",
    "        \n",
    "        # Get predicted class by taking argmax (index of the maximum logit)\n",
    "        task1_pred = torch.argmax(task1_output,1 )  # Predicted class for task 1\n",
    "        task2_pred = torch.argmax(task2_output,1)  # Predicted class for task 2\n",
    "        \n",
    "        return task1_pred, task2_pred\n",
    "        # return task1_output, task2_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "488c5a01-b097-4642-b87b-438934924626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Output: torch.Size([10])\n",
      "Task 2 Output: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_classes_task1 = 5  # 5 emotion classes\n",
    "num_classes_task2 = 11 # 11 dog classifcation classes\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiTaskResNet50(num_classes_task1, num_classes_task2)\n",
    "\n",
    "# Example input tensor with shape [batch_size, 3, 224, 224]\n",
    "input_tensor = np.random.rand(10, 3, 224, 224)  # Batch size of 2\n",
    "\n",
    "# Forward pass\n",
    "task1_output, task2_output = model(transform_input_resnet(X_train[:10]))\n",
    "\n",
    "print(\"Task 1 Output:\", task1_output.shape)  # Should be [batch_size, num_classes_task1]\n",
    "print(\"Task 2 Output:\", task2_output.shape)  # Should be [batch_size, num_classes_task2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2daf8be5-a431-4c53-949b-d0c8eb086e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 32, 32, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66e07ccd-27c9-4318-9966-265995ce5a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "779add84-17aa-460e-8b3a-78720412454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10,  9,  6, 10, 10, 10, 10,  9, 10])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edd8c-9af1-4591-8a73-1729e3cf589b",
   "metadata": {},
   "source": [
    "## Define code for actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1128e6d-767f-4fb1-8e84-be9b45f5918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 32, 32, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d58b81bc-a56f-432e-a70d-868ed2618965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0426eaad-4970-4a4a-b380-8ab6af110d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706fc43-227f-49b1-9b43-efe4b616b1db",
   "metadata": {},
   "source": [
    "### Test model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa5ede14-6f31-4687-b09f-135fb476af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = transform_input_resnet(X_train1[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acd87080-5c13-4619-887a-c1e193f65251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4556656c-66a7-49f7-a018-47f1d363eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 2]), tensor([ 9,  6, 10, 10]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ecfae-18ef-451f-a14f-d25817a0d6ee",
   "metadata": {},
   "source": [
    "First tensor is emotion classification, second tensor is dog breed classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ae85ea8-6ec2-4cf8-a909-e9d952154803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 10],\n",
       "       [10,  6],\n",
       "       [10,  1],\n",
       "       [10,  0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49f4fe3e-4d0b-4ded-a51a-2af8307303cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 10, 10, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1[0:4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e7553-2827-4912-bf06-6af98ab1f593",
   "metadata": {},
   "source": [
    "The syntax above get all the first element, which is emotion classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95d2353a-7b94-495b-8b92-abe8b3f51145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n",
      "tensor([ 9, 10])\n",
      "[[ 3 10]\n",
      " [10  6]]\n",
      "[ 3 10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up optimizer (Adam in this case)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Set up loss function (CrossEntropyLoss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer.zero_grad()  # Clear gradients from previous step\n",
    "\n",
    "# Forward pass: compute predicted logits\n",
    "emotion_pred, bred_pred = model(transform_input_resnet(X_train1[0:2]))\n",
    "\n",
    "print(emotion_pred)\n",
    "print(bred_pred)\n",
    "print(y_train1[0:2])\n",
    "print(y_train1[0:2, 0])\n",
    "# Compute loss\n",
    "loss = criterion(emotion_pred.float(), torch.tensor(y_train1[0:2, 0]).float())\n",
    "\n",
    "# Backward pass: compute gradients\n",
    "loss.requires_grad = True\n",
    "loss.backward()\n",
    "\n",
    "# Update model parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3b719-1726-46f8-928b-fc04b39ec035",
   "metadata": {},
   "source": [
    "emotion_pred compared with y_train1[:, 0] while bred_pred compared with y_train1[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d34e6-61c7-4d8a-8c6f-e1d63a9b7250",
   "metadata": {},
   "source": [
    "## Actual training happen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0102e06-c7d0-4345-bbf0-e3f9d452b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37c30fc0-8daf-413d-afa4-24f020cc7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9800c81d-41da-4120-a7d4-4e19b01014ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Emotion loss: 0.1525, Breed Loss: 0.1626, Total Loss: 0.3151\n",
      "Epoch [5/300], Emotion loss: 0.1386, Breed Loss: 0.1540, Total Loss: 0.2926\n",
      "Epoch [9/300], Emotion loss: 0.1386, Breed Loss: 0.1104, Total Loss: 0.2491\n",
      "Epoch [13/300], Emotion loss: 0.1017, Breed Loss: 0.1574, Total Loss: 0.2590\n",
      "Epoch [17/300], Emotion loss: 0.1433, Breed Loss: 0.1372, Total Loss: 0.2804\n",
      "Epoch [21/300], Emotion loss: 0.1017, Breed Loss: 0.1585, Total Loss: 0.2601\n",
      "Epoch [25/300], Emotion loss: 0.0739, Breed Loss: 0.1750, Total Loss: 0.2489\n",
      "Epoch [29/300], Emotion loss: 0.1525, Breed Loss: 0.1120, Total Loss: 0.2645\n",
      "Epoch [33/300], Emotion loss: 0.0693, Breed Loss: 0.1912, Total Loss: 0.2605\n",
      "Epoch [37/300], Emotion loss: 0.1155, Breed Loss: 0.1674, Total Loss: 0.2829\n",
      "Epoch [41/300], Emotion loss: 0.1525, Breed Loss: 0.1506, Total Loss: 0.3030\n",
      "Epoch [45/300], Emotion loss: 0.1479, Breed Loss: 0.1556, Total Loss: 0.3035\n",
      "Epoch [49/300], Emotion loss: 0.0693, Breed Loss: 0.1410, Total Loss: 0.2103\n",
      "Epoch [53/300], Emotion loss: 0.1479, Breed Loss: 0.1195, Total Loss: 0.2674\n",
      "Epoch [57/300], Emotion loss: 0.1848, Breed Loss: 0.0688, Total Loss: 0.2536\n",
      "Epoch [61/300], Emotion loss: 0.1063, Breed Loss: 0.1674, Total Loss: 0.2736\n",
      "Epoch [65/300], Emotion loss: 0.1063, Breed Loss: 0.2748, Total Loss: 0.3811\n",
      "Epoch [69/300], Emotion loss: 0.0462, Breed Loss: 0.2826, Total Loss: 0.3288\n",
      "Epoch [73/300], Emotion loss: 0.0739, Breed Loss: 0.1474, Total Loss: 0.2213\n",
      "Epoch [77/300], Emotion loss: 0.1155, Breed Loss: 0.1372, Total Loss: 0.2528\n",
      "Epoch [81/300], Emotion loss: 0.0970, Breed Loss: 0.1239, Total Loss: 0.2209\n",
      "Epoch [85/300], Emotion loss: 0.0601, Breed Loss: 0.2968, Total Loss: 0.3569\n",
      "Epoch [89/300], Emotion loss: 0.1017, Breed Loss: 0.1237, Total Loss: 0.2253\n",
      "Epoch [93/300], Emotion loss: 0.1479, Breed Loss: 0.1072, Total Loss: 0.2550\n",
      "Epoch [97/300], Emotion loss: 0.0739, Breed Loss: 0.2437, Total Loss: 0.3176\n",
      "Epoch [101/300], Emotion loss: 0.1063, Breed Loss: 0.1874, Total Loss: 0.2937\n",
      "Epoch [105/300], Emotion loss: 0.1386, Breed Loss: 0.1104, Total Loss: 0.2491\n",
      "Epoch [109/300], Emotion loss: 0.0739, Breed Loss: 0.1774, Total Loss: 0.2513\n",
      "Epoch [113/300], Emotion loss: 0.0970, Breed Loss: 0.1306, Total Loss: 0.2276\n",
      "Epoch [117/300], Emotion loss: 0.0555, Breed Loss: 0.1607, Total Loss: 0.2162\n",
      "Epoch [121/300], Emotion loss: 0.0323, Breed Loss: 0.2826, Total Loss: 0.3150\n",
      "Epoch [125/300], Emotion loss: 0.0739, Breed Loss: 0.1540, Total Loss: 0.2280\n",
      "Epoch [129/300], Emotion loss: 0.1479, Breed Loss: 0.1264, Total Loss: 0.2743\n",
      "Epoch [133/300], Emotion loss: 0.0601, Breed Loss: 0.2656, Total Loss: 0.3256\n",
      "Epoch [137/300], Emotion loss: 0.0370, Breed Loss: 0.2009, Total Loss: 0.2378\n",
      "Epoch [141/300], Emotion loss: 0.0970, Breed Loss: 0.1540, Total Loss: 0.2510\n",
      "Epoch [145/300], Emotion loss: 0.1155, Breed Loss: 0.1452, Total Loss: 0.2607\n",
      "Epoch [149/300], Emotion loss: 0.0647, Breed Loss: 0.2581, Total Loss: 0.3228\n",
      "Epoch [153/300], Emotion loss: 0.1063, Breed Loss: 0.1707, Total Loss: 0.2770\n",
      "Epoch [157/300], Emotion loss: 0.0647, Breed Loss: 0.2710, Total Loss: 0.3357\n",
      "Epoch [161/300], Emotion loss: 0.0693, Breed Loss: 0.1707, Total Loss: 0.2400\n",
      "Epoch [165/300], Emotion loss: 0.1063, Breed Loss: 0.1306, Total Loss: 0.2369\n",
      "Epoch [169/300], Emotion loss: 0.1017, Breed Loss: 0.1178, Total Loss: 0.2194\n",
      "Epoch [173/300], Emotion loss: 0.1479, Breed Loss: 0.1109, Total Loss: 0.2588\n",
      "Epoch [177/300], Emotion loss: 0.0601, Breed Loss: 0.1710, Total Loss: 0.2310\n",
      "Epoch [181/300], Emotion loss: 0.1386, Breed Loss: 0.1038, Total Loss: 0.2424\n",
      "Epoch [185/300], Emotion loss: 0.1386, Breed Loss: 0.1206, Total Loss: 0.2592\n",
      "Epoch [189/300], Emotion loss: 0.0693, Breed Loss: 0.1917, Total Loss: 0.2610\n",
      "Epoch [193/300], Emotion loss: 0.1017, Breed Loss: 0.1212, Total Loss: 0.2229\n",
      "Epoch [197/300], Emotion loss: 0.1848, Breed Loss: 0.0669, Total Loss: 0.2518\n",
      "Epoch [201/300], Emotion loss: 0.1155, Breed Loss: 0.1172, Total Loss: 0.2327\n",
      "Epoch [205/300], Emotion loss: 0.1525, Breed Loss: 0.1138, Total Loss: 0.2663\n",
      "Epoch [209/300], Emotion loss: 0.1525, Breed Loss: 0.1206, Total Loss: 0.2730\n",
      "Epoch [213/300], Emotion loss: 0.0693, Breed Loss: 0.1868, Total Loss: 0.2561\n",
      "Epoch [217/300], Emotion loss: 0.1525, Breed Loss: 0.1239, Total Loss: 0.2764\n",
      "Epoch [221/300], Emotion loss: 0.0601, Breed Loss: 0.1740, Total Loss: 0.2341\n",
      "Epoch [225/300], Emotion loss: 0.1848, Breed Loss: 0.1171, Total Loss: 0.3020\n",
      "Epoch [229/300], Emotion loss: 0.0231, Breed Loss: 0.2826, Total Loss: 0.3057\n",
      "Epoch [233/300], Emotion loss: 0.1433, Breed Loss: 0.1345, Total Loss: 0.2778\n",
      "Epoch [237/300], Emotion loss: 0.1063, Breed Loss: 0.1340, Total Loss: 0.2403\n",
      "Epoch [241/300], Emotion loss: 0.0739, Breed Loss: 0.1410, Total Loss: 0.2150\n",
      "Epoch [245/300], Emotion loss: 0.1386, Breed Loss: 0.1571, Total Loss: 0.2957\n",
      "Epoch [249/300], Emotion loss: 0.1155, Breed Loss: 0.1205, Total Loss: 0.2361\n",
      "Epoch [253/300], Emotion loss: 0.1433, Breed Loss: 0.1507, Total Loss: 0.2940\n",
      "Epoch [257/300], Emotion loss: 0.0739, Breed Loss: 0.2693, Total Loss: 0.3432\n",
      "Epoch [261/300], Emotion loss: 0.0693, Breed Loss: 0.1607, Total Loss: 0.2300\n",
      "Epoch [265/300], Emotion loss: 0.1063, Breed Loss: 0.1041, Total Loss: 0.2104\n",
      "Epoch [269/300], Emotion loss: 0.1201, Breed Loss: 0.1410, Total Loss: 0.2612\n",
      "Epoch [273/300], Emotion loss: 0.1433, Breed Loss: 0.1521, Total Loss: 0.2954\n",
      "Epoch [277/300], Emotion loss: 0.0647, Breed Loss: 0.1410, Total Loss: 0.2057\n",
      "Epoch [281/300], Emotion loss: 0.1017, Breed Loss: 0.1573, Total Loss: 0.2589\n",
      "Epoch [285/300], Emotion loss: 0.1017, Breed Loss: 0.1306, Total Loss: 0.2323\n",
      "Epoch [289/300], Emotion loss: 0.1848, Breed Loss: 0.1172, Total Loss: 0.3021\n",
      "Epoch [293/300], Emotion loss: 0.0555, Breed Loss: 0.1975, Total Loss: 0.2530\n",
      "Epoch [297/300], Emotion loss: 0.1201, Breed Loss: 0.1287, Total Loss: 0.2488\n"
     ]
    }
   ],
   "source": [
    "# Set up loss function (CrossEntropyLoss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "emotion_loss_fn = nn.CrossEntropyLoss()  # For classification task\n",
    "breed_loss_fn = nn.CrossEntropyLoss()  # For regression task\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "for i in range(0, X_train1.shape[0], batch_size):\n",
    "    running_emotion_loss = 0.0\n",
    "    running_breed_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "    \n",
    "    batch_input = X_train1[i:i+batch_size]\n",
    "    batch_label = y_train1[i:i+batch_size]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_input = transform_input_resnet(batch_input)\n",
    "    emotion_pred, breed_pred = model(batch_input)\n",
    "\n",
    "    emotion_loss = criterion(emotion_pred.float(), torch.tensor(batch_label[:, 0]).float())\n",
    "    breed_loss = criterion(breed_pred.float(), torch.tensor(batch_label[:, 1]).float())\n",
    "\n",
    "    emotion_loss.requires_grad = True\n",
    "    breed_loss.requires_grad = True\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = emotion_loss + breed_loss\n",
    "\n",
    "    # Backpropagate the loss and update weights\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumulate the losses for reporting\n",
    "    running_emotion_loss += emotion_loss.item()\n",
    "    running_breed_loss += breed_loss.item()\n",
    "    running_total_loss += total_loss.item()\n",
    "    # Print the average losses for this epoch\n",
    "    avg_emotion_loss = running_emotion_loss/train_size\n",
    "    avg_breed_loss = running_breed_loss/train_size\n",
    "    avg_total_loss = running_total_loss / train_size\n",
    "\n",
    "    print(f\"Epoch [{i+1}/{X_train1.shape[0]}], \"\n",
    "          f\"Emotion loss: {avg_emotion_loss:.4f}, \"\n",
    "          f\"Breed Loss: {avg_breed_loss:.4f}, \"\n",
    "          f\"Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "661eb365-ad18-4200-b87b-a775d5ceac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e98a3fe6-87b9-49e8-99d7-319910a229d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train is 77.34147357940674 seconds\n"
     ]
    }
   ],
   "source": [
    "duration = end_time - start_time\n",
    "print(\"Time to train is \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c10f62b-43ae-46f2-bf06-d8a5873eba5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 32, 32, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f6a87da-e7bc-4003-be5a-e66e6346bbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "70d4a9c1-ece0-4662-ae9b-972257378083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f193f8b8-2df7-4896-b620-9aef16d8df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_pred, breed_pred = model(transform_input_resnet(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2cf700b-74c5-49ad-80d3-bdafe6a34124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3072611e-fff7-444c-9412-8cc2e4e9eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 10, 10, 10,  6,  9, 10, 10,  2,  9, 10,  9,  9, 10,  9, 10, 10,  9,\n",
       "         9, 10])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e312f4aa-9ebe-456b-a3ce-75cae7192d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:, 0], emotion_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45351061-4d2d-4ce5-a4ea-4b296e5ff227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:, 1], breed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e81c39c-b122-4e13-a73d-cc78dff4f3c0",
   "metadata": {},
   "source": [
    "The model performs poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef5b4f-1a17-44ac-9122-5a9082519855",
   "metadata": {},
   "source": [
    "## Model below doesn't incorporate breed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5918925-0943-4b1d-a36d-164e470febfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac18b290-414a-4bc9-b0d6-fec540964ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68c856a3-c1be-4fca-b466-67898cb4b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Emotion loss: 0.1525, Breed Loss: 0.1626, Total Loss: 0.1525\n",
      "Epoch [5/300], Emotion loss: 0.1386, Breed Loss: 0.1540, Total Loss: 0.1386\n",
      "Epoch [9/300], Emotion loss: 0.1386, Breed Loss: 0.1104, Total Loss: 0.1386\n",
      "Epoch [13/300], Emotion loss: 0.1017, Breed Loss: 0.1574, Total Loss: 0.1017\n",
      "Epoch [17/300], Emotion loss: 0.1433, Breed Loss: 0.1372, Total Loss: 0.1433\n",
      "Epoch [21/300], Emotion loss: 0.1017, Breed Loss: 0.1585, Total Loss: 0.1017\n",
      "Epoch [25/300], Emotion loss: 0.0739, Breed Loss: 0.1750, Total Loss: 0.0739\n",
      "Epoch [29/300], Emotion loss: 0.1525, Breed Loss: 0.1120, Total Loss: 0.1525\n",
      "Epoch [33/300], Emotion loss: 0.0693, Breed Loss: 0.1912, Total Loss: 0.0693\n",
      "Epoch [37/300], Emotion loss: 0.1155, Breed Loss: 0.1674, Total Loss: 0.1155\n",
      "Epoch [41/300], Emotion loss: 0.1525, Breed Loss: 0.1506, Total Loss: 0.1525\n",
      "Epoch [45/300], Emotion loss: 0.1479, Breed Loss: 0.1556, Total Loss: 0.1479\n",
      "Epoch [49/300], Emotion loss: 0.0693, Breed Loss: 0.1410, Total Loss: 0.0693\n",
      "Epoch [53/300], Emotion loss: 0.1479, Breed Loss: 0.1195, Total Loss: 0.1479\n",
      "Epoch [57/300], Emotion loss: 0.1848, Breed Loss: 0.0688, Total Loss: 0.1848\n",
      "Epoch [61/300], Emotion loss: 0.1063, Breed Loss: 0.1674, Total Loss: 0.1063\n",
      "Epoch [65/300], Emotion loss: 0.1063, Breed Loss: 0.2748, Total Loss: 0.1063\n",
      "Epoch [69/300], Emotion loss: 0.0462, Breed Loss: 0.2826, Total Loss: 0.0462\n",
      "Epoch [73/300], Emotion loss: 0.0739, Breed Loss: 0.1474, Total Loss: 0.0739\n",
      "Epoch [77/300], Emotion loss: 0.1155, Breed Loss: 0.1372, Total Loss: 0.1155\n",
      "Epoch [81/300], Emotion loss: 0.0970, Breed Loss: 0.1239, Total Loss: 0.0970\n",
      "Epoch [85/300], Emotion loss: 0.0601, Breed Loss: 0.2968, Total Loss: 0.0601\n",
      "Epoch [89/300], Emotion loss: 0.1017, Breed Loss: 0.1237, Total Loss: 0.1017\n",
      "Epoch [93/300], Emotion loss: 0.1479, Breed Loss: 0.1072, Total Loss: 0.1479\n",
      "Epoch [97/300], Emotion loss: 0.0739, Breed Loss: 0.2437, Total Loss: 0.0739\n",
      "Epoch [101/300], Emotion loss: 0.1063, Breed Loss: 0.1874, Total Loss: 0.1063\n",
      "Epoch [105/300], Emotion loss: 0.1386, Breed Loss: 0.1104, Total Loss: 0.1386\n",
      "Epoch [109/300], Emotion loss: 0.0739, Breed Loss: 0.1774, Total Loss: 0.0739\n",
      "Epoch [113/300], Emotion loss: 0.0970, Breed Loss: 0.1306, Total Loss: 0.0970\n",
      "Epoch [117/300], Emotion loss: 0.0555, Breed Loss: 0.1607, Total Loss: 0.0555\n",
      "Epoch [121/300], Emotion loss: 0.0323, Breed Loss: 0.2826, Total Loss: 0.0323\n",
      "Epoch [125/300], Emotion loss: 0.0739, Breed Loss: 0.1540, Total Loss: 0.0739\n",
      "Epoch [129/300], Emotion loss: 0.1479, Breed Loss: 0.1264, Total Loss: 0.1479\n",
      "Epoch [133/300], Emotion loss: 0.0601, Breed Loss: 0.2656, Total Loss: 0.0601\n",
      "Epoch [137/300], Emotion loss: 0.0370, Breed Loss: 0.2009, Total Loss: 0.0370\n",
      "Epoch [141/300], Emotion loss: 0.0970, Breed Loss: 0.1540, Total Loss: 0.0970\n",
      "Epoch [145/300], Emotion loss: 0.1155, Breed Loss: 0.1452, Total Loss: 0.1155\n",
      "Epoch [149/300], Emotion loss: 0.0647, Breed Loss: 0.2581, Total Loss: 0.0647\n",
      "Epoch [153/300], Emotion loss: 0.1063, Breed Loss: 0.1707, Total Loss: 0.1063\n",
      "Epoch [157/300], Emotion loss: 0.0647, Breed Loss: 0.2710, Total Loss: 0.0647\n",
      "Epoch [161/300], Emotion loss: 0.0693, Breed Loss: 0.1707, Total Loss: 0.0693\n",
      "Epoch [165/300], Emotion loss: 0.1063, Breed Loss: 0.1306, Total Loss: 0.1063\n",
      "Epoch [169/300], Emotion loss: 0.1017, Breed Loss: 0.1178, Total Loss: 0.1017\n",
      "Epoch [173/300], Emotion loss: 0.1479, Breed Loss: 0.1109, Total Loss: 0.1479\n",
      "Epoch [177/300], Emotion loss: 0.0601, Breed Loss: 0.1710, Total Loss: 0.0601\n",
      "Epoch [181/300], Emotion loss: 0.1386, Breed Loss: 0.1038, Total Loss: 0.1386\n",
      "Epoch [185/300], Emotion loss: 0.1386, Breed Loss: 0.1206, Total Loss: 0.1386\n",
      "Epoch [189/300], Emotion loss: 0.0693, Breed Loss: 0.1917, Total Loss: 0.0693\n",
      "Epoch [193/300], Emotion loss: 0.1017, Breed Loss: 0.1212, Total Loss: 0.1017\n",
      "Epoch [197/300], Emotion loss: 0.1848, Breed Loss: 0.0669, Total Loss: 0.1848\n",
      "Epoch [201/300], Emotion loss: 0.1155, Breed Loss: 0.1172, Total Loss: 0.1155\n",
      "Epoch [205/300], Emotion loss: 0.1525, Breed Loss: 0.1138, Total Loss: 0.1525\n",
      "Epoch [209/300], Emotion loss: 0.1525, Breed Loss: 0.1206, Total Loss: 0.1525\n",
      "Epoch [213/300], Emotion loss: 0.0693, Breed Loss: 0.1868, Total Loss: 0.0693\n",
      "Epoch [217/300], Emotion loss: 0.1525, Breed Loss: 0.1239, Total Loss: 0.1525\n",
      "Epoch [221/300], Emotion loss: 0.0601, Breed Loss: 0.1740, Total Loss: 0.0601\n",
      "Epoch [225/300], Emotion loss: 0.1848, Breed Loss: 0.1171, Total Loss: 0.1848\n",
      "Epoch [229/300], Emotion loss: 0.0231, Breed Loss: 0.2826, Total Loss: 0.0231\n",
      "Epoch [233/300], Emotion loss: 0.1433, Breed Loss: 0.1345, Total Loss: 0.1433\n",
      "Epoch [237/300], Emotion loss: 0.1063, Breed Loss: 0.1340, Total Loss: 0.1063\n",
      "Epoch [241/300], Emotion loss: 0.0739, Breed Loss: 0.1410, Total Loss: 0.0739\n",
      "Epoch [245/300], Emotion loss: 0.1386, Breed Loss: 0.1571, Total Loss: 0.1386\n",
      "Epoch [249/300], Emotion loss: 0.1155, Breed Loss: 0.1205, Total Loss: 0.1155\n",
      "Epoch [253/300], Emotion loss: 0.1433, Breed Loss: 0.1507, Total Loss: 0.1433\n",
      "Epoch [257/300], Emotion loss: 0.0739, Breed Loss: 0.2693, Total Loss: 0.0739\n",
      "Epoch [261/300], Emotion loss: 0.0693, Breed Loss: 0.1607, Total Loss: 0.0693\n",
      "Epoch [265/300], Emotion loss: 0.1063, Breed Loss: 0.1041, Total Loss: 0.1063\n",
      "Epoch [269/300], Emotion loss: 0.1201, Breed Loss: 0.1410, Total Loss: 0.1201\n",
      "Epoch [273/300], Emotion loss: 0.1433, Breed Loss: 0.1521, Total Loss: 0.1433\n",
      "Epoch [277/300], Emotion loss: 0.0647, Breed Loss: 0.1410, Total Loss: 0.0647\n",
      "Epoch [281/300], Emotion loss: 0.1017, Breed Loss: 0.1573, Total Loss: 0.1017\n",
      "Epoch [285/300], Emotion loss: 0.1017, Breed Loss: 0.1306, Total Loss: 0.1017\n",
      "Epoch [289/300], Emotion loss: 0.1848, Breed Loss: 0.1172, Total Loss: 0.1848\n",
      "Epoch [293/300], Emotion loss: 0.0555, Breed Loss: 0.1975, Total Loss: 0.0555\n",
      "Epoch [297/300], Emotion loss: 0.1201, Breed Loss: 0.1287, Total Loss: 0.1201\n"
     ]
    }
   ],
   "source": [
    "emotion_loss_fn = nn.CrossEntropyLoss()  # For classification task\n",
    "breed_loss_fn = nn.CrossEntropyLoss()  # For regression task\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "for i in range(0, X_train1.shape[0], batch_size):\n",
    "    running_emotion_loss = 0.0\n",
    "    running_breed_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "    \n",
    "    batch_input = X_train1[i:i+batch_size]\n",
    "    batch_label = y_train1[i:i+batch_size]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_input = transform_input_resnet(batch_input)\n",
    "    emotion_pred, breed_pred = model(batch_input)\n",
    "\n",
    "    emotion_loss = criterion(emotion_pred.float(), torch.tensor(batch_label[:, 0]).float())\n",
    "    breed_loss = criterion(breed_pred.float(), torch.tensor(batch_label[:, 1]).float())\n",
    "\n",
    "    emotion_loss.requires_grad = True\n",
    "    breed_loss.requires_grad = True\n",
    "\n",
    "    # Combine the losses\n",
    "    # total_loss = emotion_loss + breed_loss\n",
    "    total_loss = emotion_loss\n",
    "\n",
    "    # Backpropagate the loss and update weights\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accumulate the losses for reporting\n",
    "    running_emotion_loss += emotion_loss.item()\n",
    "    running_breed_loss += breed_loss.item()\n",
    "    running_total_loss += total_loss.item()\n",
    "    # Print the average losses for this epoch\n",
    "    avg_emotion_loss = running_emotion_loss/train_size\n",
    "    avg_breed_loss = running_breed_loss/train_size\n",
    "    avg_total_loss = running_total_loss / train_size\n",
    "\n",
    "    print(f\"Epoch [{i+1}/{X_train1.shape[0]}], \"\n",
    "          f\"Emotion loss: {avg_emotion_loss:.4f}, \"\n",
    "          f\"Breed Loss: {avg_breed_loss:.4f}, \"\n",
    "          f\"Total Loss: {avg_total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24fb8357-24a2-41a9-bb17-0ec79cddafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df245081-dd9d-410f-8377-d74b804ad73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train is 74.80636024475098 seconds\n"
     ]
    }
   ],
   "source": [
    "duration = end_time - start_time\n",
    "print(\"Time to train is \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "270b65d1-69f7-49de-85c6-5afdfd56c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_pred, breed_pred = model(transform_input_resnet(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8bfab901-85cb-481f-a030-da3fd7046ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "358bbd95-c27a-47b6-bdfc-0fb05f795f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 10, 10, 10,  6,  9, 10, 10,  2,  9, 10,  9,  9, 10,  9, 10, 10,  9,\n",
       "         9, 10])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7ac155d-ebc2-4c4a-b0af-7f44a9d8e1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:, 0], emotion_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ce669a0-63f1-4372-8f32-361546f136d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[:, 1], breed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa6235-2d74-4393-8b8b-1f27b5c5f9c5",
   "metadata": {},
   "source": [
    "The model performs poorly as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
