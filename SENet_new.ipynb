{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62bf35-b4b2-43f1-a08a-7bc4a680c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324ba1-732e-488f-94c5-4116625986fd",
   "metadata": {},
   "source": [
    "## Load dog breed and dog classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947df478-2079-48e7-8672-df29cc2a6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data(data_dir):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            # print(img)\n",
    "            # print(path)\n",
    "            # img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            # print(os.path.join(path,img))\n",
    "            if img_arr is None:\n",
    "                # print(f\"Warning: Unable to load image {img}\")\n",
    "                print(os.path.join(path,img))\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "            \n",
    "    return np.array(data), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8dc13f-3140-49df-b8ad-85c528cbc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data_subset(data_dir, subset_size=None):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)  # Assuming label directories are in data_dir\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        \n",
    "        # Get list of images in the directory\n",
    "        all_images = os.listdir(path)\n",
    "        \n",
    "        # If subset_size is provided, select a random subset of images\n",
    "        if subset_size is not None and len(all_images) > subset_size:\n",
    "            all_images = random.sample(all_images, subset_size)\n",
    "        \n",
    "        for img in all_images:\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            if img_arr is None:\n",
    "                print(f\"Warning: Unable to load image {img}\")\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            \n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "    \n",
    "    return np.array(data), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbbc6f4a-4e17-4f04-b279-7e3a5c127e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_resnet(data):\n",
    "    # Convert teh data array to a PIL Image\n",
    "    \n",
    "    # Example: Load multiple images (let's say 3 images)\n",
    "    # image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]  # List of image file paths\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),           # Resize to 256x256 (larger than 224x224 to allow for cropping)\n",
    "        transforms.CenterCrop(224),       # Crop to 224x224\n",
    "        transforms.ToTensor(),            # Convert to tensor (will scale pixel values to [0, 1])\n",
    "        transforms.Normalize(             # Normalize with ImageNet mean and std\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Prepare the list of images\n",
    "    input_tensors = []\n",
    "    for image in data:\n",
    "        image = Image.fromarray(image)\n",
    "        image_tensor = transform(image)  # Apply transformations\n",
    "        input_tensors.append(image_tensor)\n",
    "    \n",
    "    # Stack the images into a single tensor (batch of images)\n",
    "    input_batch = torch.stack(input_tensors)  # Shape will be [batch_size, 3, 224, 224]\n",
    "    return input_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b679c2-6d7b-4f13-950b-cb1714a5f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for image categories\n",
    "labels = ['Beagle', 'Boxer', 'Bulldog', 'Dachshund', 'German_Shepherd', 'Golden_Retriever', 'Labrador_Retriever', 'Poodle', 'Rottweiler', 'Yorkshire_Terrier']\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb29e7a-9a1a-456d-bf1b-c478768e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed_train_data, dog_breed_train_label = loading_training_data('./DogBreedImageDataset/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3112deea-d087-4d3f-bd80-aa510aa039a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fdea5f-06b1-4477-a7a0-b8a0c980bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dog_breed_train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856b0137-57a5-4251-870a-0590d32d7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ed1a08-76b8-4839-8634-0accc75ba150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db817a6-1d5f-4f12-9e68-1ab91c7008f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transform_input_resnet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7fca79-d629-4b4c-8617-73f0a0e9a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb0e35b-c515-46d6-85b4-c694ac41366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['angry', 'happy', 'relaxed', 'sad']\n",
    "# Load data for training\n",
    "dog_emotion_train_data, dog_emotion_train_label = loading_training_data_subset('./DogEmotionPrediction/images/', 250) #100 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46b8c56-ee2e-447d-847a-c7fe16fe1f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b106717b-04e6-40ad-87f9-204fc20b299d",
   "metadata": {},
   "source": [
    "## Combine the two together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2d2824c-7548-4632-a7c7-6a3fbcc05b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfe452f8-bd61-4abd-8bd9-1c9e08d4699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3daf7-7ad5-4ec6-89db-38450c61f16d",
   "metadata": {},
   "source": [
    "### Modify label to have two label, first label is emotion, second is dog breed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dac9b8a-e625-4032-a502-6188dfa9daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed_train_label = dog_breed_train_label.reshape(-1, 1)\n",
    "\n",
    "# Create a column of zeros (same number of rows as arr_reshaped)\n",
    "emotion_col = np.full((dog_breed_train_label.shape[0], 1), 10)\n",
    "\n",
    "# Add the column of zeros to the right using np.hstack\n",
    "dog_breed_train_label1 = np.hstack((emotion_col, dog_breed_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b090977-1109-4ccb-9ba3-acd80fc8f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_emotion_train_label = dog_emotion_train_label.reshape(-1, 1)\n",
    "\n",
    "# Create a column of zeros (same number of rows as arr_reshaped)\n",
    "breed_col = np.full((dog_emotion_train_label.shape[0], 1), 10)\n",
    "\n",
    "# Add the column of zeros to the right using np.hstack\n",
    "dog_emotion_train_label1 = np.hstack((dog_emotion_train_label, breed_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "760bae57-6bbc-4615-a6ce-92deb8ffb139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_label1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7485de9-c794-4b8c-b90e-cc894b708d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  0],\n",
       "       [10,  0],\n",
       "       [10,  0],\n",
       "       ...,\n",
       "       [10,  9],\n",
       "       [10,  9],\n",
       "       [10,  9]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57cfde72-2c2f-4cf8-bc8e-d1fc51239779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_label1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ad0d8fb-bee5-40ed-becd-8ba7216aeb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 10],\n",
       "       [ 0, 10],\n",
       "       [ 0, 10],\n",
       "       ...,\n",
       "       [ 3, 10],\n",
       "       [ 3, 10],\n",
       "       [ 3, 10]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_label1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "172118e1-d2c5-4c64-a208-103be5d2a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = np.concatenate((dog_breed_train_data, dog_emotion_train_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12c72f74-9fdd-4e56-9d3e-a5560eac95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_label = np.concatenate((dog_breed_train_label1, dog_emotion_train_label1), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f164c9e0-edd3-488c-9abc-7405c1ec3174",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b56b2c8d-3299-4afa-9cd4-45a7035a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fea0efa2-b9f4-44c7-bee8-445f5ac4bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(combined_dataset, combined_label, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17fd4f8e-c6f5-4d7e-b4f9-8df49f82399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1180, 32, 32, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "492d9cc8-62d9-41f5-bb38-5e3eefe1b333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb6d73-11d8-4666-a7fd-df59dbd72ccc",
   "metadata": {},
   "source": [
    "### Take part of data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "130312db-ff46-4578-ba08-d6d30bb25875",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_indices = np.random.choice(X_train.shape[0], 50, replace=False)\n",
    "\n",
    "# Use these indices to extract 50 random samples\n",
    "X_train1 = X_train[random_indices]\n",
    "y_train1 = y_train[random_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3847916-a0bb-4086-95a5-d9e3a3050922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 32, 32, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b20e42b-3ef6-4784-b0ec-123f75fcce51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe9a4e01-0105-4d4d-a9b4-615a741ac80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282e771-d8fa-4ed8-9428-244f59cb8551",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d81dc99a-2926-44f0-a2e2-453d1b345109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MultiTaskResNet50(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2):\n",
    "        super(MultiTaskResNet50, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50()\n",
    "        \n",
    "        # Remove the final fully connected layer (classifier) for both tasks\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Exclude final FC layer\n",
    "        \n",
    "        # Task 1: A new fully connected layer for the first task\n",
    "        self.fc_task1 = nn.Linear(2048, num_classes_task1)\n",
    "        \n",
    "        # Task 2: A new fully connected layer for the second task\n",
    "        self.fc_task2 = nn.Linear(2048, num_classes_task2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through the ResNet backbone (everything before the final fully connected layer)\n",
    "        features = self.resnet(x)  # Output size: [batch_size, 2048, 1, 1]\n",
    "        \n",
    "        # Flatten the output to [batch_size, 2048]\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Task 1: Pass through the task-specific classifier\n",
    "        task1_output = self.fc_task1(features)\n",
    "        \n",
    "        # Task 2: Pass through the task-specific classifier\n",
    "        task2_output = self.fc_task2(features)\n",
    "        \n",
    "        # Get predicted class by taking argmax (index of the maximum logit)\n",
    "        task1_pred = torch.argmax(task1_output,1 )  # Predicted class for task 1\n",
    "        task2_pred = torch.argmax(task2_output,1)  # Predicted class for task 2\n",
    "        \n",
    "        return task1_pred, task2_pred\n",
    "        # return task1_output, task2_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "488c5a01-b097-4642-b87b-438934924626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Output: torch.Size([10])\n",
      "Task 2 Output: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "num_classes_task1 = 10  # Number of classes for task 1\n",
    "num_classes_task2 = 4   # Number of classes for task 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiTaskResNet50(num_classes_task1, num_classes_task2)\n",
    "\n",
    "# Example input tensor with shape [batch_size, 3, 224, 224]\n",
    "input_tensor = np.random.rand(10, 3, 224, 224)  # Batch size of 2\n",
    "\n",
    "# Forward pass\n",
    "task1_output, task2_output = model(transform_input_resnet(X_train[:10]))\n",
    "\n",
    "print(\"Task 1 Output:\", task1_output.shape)  # Should be [batch_size, num_classes_task1]\n",
    "print(\"Task 2 Output:\", task2_output.shape)  # Should be [batch_size, num_classes_task2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2daf8be5-a431-4c53-949b-d0c8eb086e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 32, 32, 3)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "66e07ccd-27c9-4318-9966-265995ce5a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 5, 9, 4, 4, 4, 4, 5, 5])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "779add84-17aa-460e-8b3a-78720412454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00edd8c-9af1-4591-8a73-1729e3cf589b",
   "metadata": {},
   "source": [
    "## Define code for actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1128e6d-767f-4fb1-8e84-be9b45f5918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 32, 32, 3)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d58b81bc-a56f-432e-a70d-868ed2618965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0426eaad-4970-4a4a-b380-8ab6af110d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6ee354c8-f0ed-4bdc-9a99-b66e7b93b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa5ede14-6f31-4687-b09f-135fb476af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = transform_input_resnet(X_train1[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "acd87080-5c13-4619-887a-c1e193f65251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4556656c-66a7-49f7-a018-47f1d363eacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]==38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9800c81d-41da-4120-a7d4-4e19b01014ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([5, 4, 4, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 5, 4, 2]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 4, 2, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 9, 2, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 4, 4, 2]), tensor([2, 2, 2, 2]))\n",
      "(tensor([5, 2, 5, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([9, 4, 5, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([2, 5, 4, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 2, 4, 2]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 4, 5, 5]), tensor([2, 2, 2, 2]))\n",
      "(tensor([5, 4, 4, 5]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 5, 4, 4]), tensor([2, 2, 2, 2]))\n",
      "(tensor([4, 4]), tensor([2, 2]))\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, X_train1.shape[0], batch_size):\n",
    "    batch_input = X_train1[i:i+batch_size]\n",
    "    batch_label = y_train1[i:i+batch_size]\n",
    "    batch_input = transform_input_resnet(batch_input)\n",
    "    prediction = model(batch_input)\n",
    "    print(prediction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e507bb56-8505-4b79-b51c-6edacd27df3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 5, 1]), tensor([0, 0, 2]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(transform_input_resnet(X_train1[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bbc2642a-feda-45f6-bb5a-bd94bbe78e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khain\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha\n",
      "tensor([10,  1], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m emotion_output, breed_output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Compute the classification and regression losses\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m emotion_loss \u001b[38;5;241m=\u001b[39m \u001b[43memotion_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memotion_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotion_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m breed_loss \u001b[38;5;241m=\u001b[39m breed_loss_fn(breed_output, breed_target)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Combine the losses (can use weighted sum if needed)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Int"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the Loss Functions\n",
    "emotion_loss_fn = nn.CrossEntropyLoss()  # For classification task\n",
    "breed_loss_fn = nn.CrossEntropyLoss()  # For regression task\n",
    "\n",
    "# Step 4: Define the optimizer\n",
    "model = MultiTaskResNet50(5,11)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 6: Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_emotion_loss = 0.0\n",
    "    running_breed_loss = 0.0\n",
    "    running_total_loss = 0.0\n",
    "\n",
    "    for input, emotion_target, breed_target in dataloader:\n",
    "        input1 = input.long()\n",
    "        print(\"haha\")\n",
    "        print(emotion_target)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        # Forward pass\n",
    "        emotion_output, breed_output = model(input)\n",
    "\n",
    "        # Compute the classification and regression losses\n",
    "        emotion_loss = emotion_loss_fn(emotion_output, emotion_target)\n",
    "        breed_loss = breed_loss_fn(breed_output, breed_target)\n",
    "\n",
    "        # Combine the losses (can use weighted sum if needed)\n",
    "        total_loss = emotion_loss + breed_loss\n",
    "\n",
    "        # Backpropagate the loss and update weights\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the losses for reporting\n",
    "        running_emotion_loss += emotion_loss.item()\n",
    "        running_breed_loss += breed_loss.item()\n",
    "        running_total_loss += total_loss.item()\n",
    "    # Print the average losses for this epoch\n",
    "    avg_emotion_loss = running_emotion_loss/len(dataloader)\n",
    "    avg_breed_loss = running_breed_loss/len(dataloader)\n",
    "    avg_total_loss = running_total_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Emotion loss: {avg_emotion_loss:.4f}, \"\n",
    "          f\"Breed Loss: {avg_breed_loss:.4f}, \"\n",
    "          f\"Total Loss: {avg_total_loss:.4f}\")\n",
    "\n",
    "# Step 7: Save the trained model (optional)\n",
    "torch.save(model.state_dict(), 'multi_task_resnet.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90192c-424b-4da7-9db2-0c3476ee21d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
