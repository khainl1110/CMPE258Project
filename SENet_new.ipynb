{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f62bf35-b4b2-43f1-a08a-7bc4a680c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca324ba1-732e-488f-94c5-4116625986fd",
   "metadata": {},
   "source": [
    "## Load dog breed and dog classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947df478-2079-48e7-8672-df29cc2a6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data(data_dir):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            # print(img)\n",
    "            # print(path)\n",
    "            # img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            # print(os.path.join(path,img))\n",
    "            if img_arr is None:\n",
    "                # print(f\"Warning: Unable to load image {img}\")\n",
    "                print(os.path.join(path,img))\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "            \n",
    "    return np.array(data), np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e8dc13f-3140-49df-b8ad-85c528cbc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_training_data_subset(data_dir, subset_size=None):\n",
    "    data = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        # path = os.path.join(data_dir, label)  # Assuming label directories are in data_dir\n",
    "        path = data_dir+\"/\"+label\n",
    "        class_num = labels.index(label)\n",
    "        \n",
    "        # Get list of images in the directory\n",
    "        all_images = os.listdir(path)\n",
    "        \n",
    "        # If subset_size is provided, select a random subset of images\n",
    "        if subset_size is not None and len(all_images) > subset_size:\n",
    "            all_images = random.sample(all_images, subset_size)\n",
    "        \n",
    "        for img in all_images:\n",
    "            img_arr = cv2.imread(os.path.join(path, img))\n",
    "            \n",
    "            if img_arr is None:\n",
    "                print(f\"Warning: Unable to load image {img}\")\n",
    "                continue  # Skip the image if it can't be loaded\n",
    "            \n",
    "            resized_arr = cv2.resize(img_arr, (img_size, img_size))\n",
    "            data.append(resized_arr)\n",
    "            labels_list.append(class_num)\n",
    "    \n",
    "    return np.array(data), np.array(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbc6f4a-4e17-4f04-b279-7e3a5c127e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_resnet(data):\n",
    "    # Convert teh data array to a PIL Image\n",
    "    \n",
    "    # Example: Load multiple images (let's say 3 images)\n",
    "    # image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]  # List of image file paths\n",
    "\n",
    "    # Define the transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),           # Resize to 256x256 (larger than 224x224 to allow for cropping)\n",
    "        transforms.CenterCrop(224),       # Crop to 224x224\n",
    "        transforms.ToTensor(),            # Convert to tensor (will scale pixel values to [0, 1])\n",
    "        transforms.Normalize(             # Normalize with ImageNet mean and std\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "    \n",
    "    # Prepare the list of images\n",
    "    input_tensors = []\n",
    "    for image in data:\n",
    "        image = Image.fromarray(image)\n",
    "        image_tensor = transform(image)  # Apply transformations\n",
    "        input_tensors.append(image_tensor)\n",
    "    \n",
    "    # Stack the images into a single tensor (batch of images)\n",
    "    input_batch = torch.stack(input_tensors)  # Shape will be [batch_size, 3, 224, 224]\n",
    "    return input_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82b679c2-6d7b-4f13-950b-cb1714a5f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for image categories\n",
    "labels = ['Beagle', 'Boxer', 'Bulldog', 'Dachshund', 'German_Shepherd', 'Golden_Retriever', 'Labrador_Retriever', 'Poodle', 'Rottweiler', 'Yorkshire_Terrier']\n",
    "img_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb29e7a-9a1a-456d-bf1b-c478768e2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_breed_train_data, dog_breed_train_label = loading_training_data('./DogBreedImageDataset/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3112deea-d087-4d3f-bd80-aa510aa039a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 32, 32, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_breed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72fdea5f-06b1-4477-a7a0-b8a0c980bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dog_breed_train_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "856b0137-57a5-4251-870a-0590d32d7ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 32, 32, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05ed1a08-76b8-4839-8634-0accc75ba150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2db817a6-1d5f-4f12-9e68-1ab91c7008f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transform_input_resnet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd7fca79-d629-4b4c-8617-73f0a0e9a6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bb0e35b-c515-46d6-85b4-c694ac41366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['angry', 'happy', 'relaxed', 'sad']\n",
    "# Load data for training\n",
    "dog_emotion_train_data, dog_emotion_train_label = loading_training_data_subset('./DogEmotionPrediction/images/', 250) #100 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d46b8c56-ee2e-447d-847a-c7fe16fe1f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_emotion_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282e771-d8fa-4ed8-9428-244f59cb8551",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d81dc99a-2926-44f0-a2e2-453d1b345109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 Output: torch.Size([5])\n",
      "Task 2 Output: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class MultiTaskResNet50(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2):\n",
    "        super(MultiTaskResNet50, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the final fully connected layer (classifier) for both tasks\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Exclude final FC layer\n",
    "        \n",
    "        # Task 1: A new fully connected layer for the first task\n",
    "        self.fc_task1 = nn.Linear(2048, num_classes_task1)\n",
    "        \n",
    "        # Task 2: A new fully connected layer for the second task\n",
    "        self.fc_task2 = nn.Linear(2048, num_classes_task2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through the ResNet backbone (everything before the final fully connected layer)\n",
    "        features = self.resnet(x)  # Output size: [batch_size, 2048, 1, 1]\n",
    "        \n",
    "        # Flatten the output to [batch_size, 2048]\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Task 1: Pass through the task-specific classifier\n",
    "        task1_output = self.fc_task1(features)\n",
    "        \n",
    "        # Task 2: Pass through the task-specific classifier\n",
    "        task2_output = self.fc_task2(features)\n",
    "        \n",
    "        # Get predicted class by taking argmax (index of the maximum logit)\n",
    "        task1_pred = torch.argmax(task1_output, dim=1)  # Predicted class for task 1\n",
    "        task2_pred = torch.argmax(task2_output, dim=1)  # Predicted class for task 2\n",
    "        \n",
    "        return task1_pred, task2_pred\n",
    "\n",
    "# Example usage:\n",
    "num_classes_task1 = 10  # Number of classes for task 1\n",
    "num_classes_task2 = 4   # Number of classes for task 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiTaskResNet50(num_classes_task1, num_classes_task2)\n",
    "\n",
    "# Example input tensor with shape [batch_size, 3, 224, 224]\n",
    "input_tensor = torch.randn(2, 3, 224, 224)  # Batch size of 2\n",
    "\n",
    "# Forward pass\n",
    "task1_output, task2_output = model(input_batch)\n",
    "\n",
    "print(\"Task 1 Output:\", task1_output.shape)  # Should be [batch_size, num_classes_task1]\n",
    "print(\"Task 2 Output:\", task2_output.shape)  # Should be [batch_size, num_classes_task2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4e6382af-aa20-443a-a44f-527a916b9f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 9, 9, 8, 9])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "779add84-17aa-460e-8b3a-78720412454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394267f-2446-448c-ab79-365afe2c9dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
